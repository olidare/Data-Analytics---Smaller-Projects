{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multiple Linear Regression Analysis: Marketing Channels Impact on Sales",
        "",
        "## Introduction",
        "",
        "In this comprehensive analysis, we'll explore how different marketing channels influence sales performance using multiple linear regression. This statistical technique helps us understand the linear relationship between one dependent variable (sales) and multiple independent variables (various marketing channels).",
        "",
        "Multiple linear regression is particularly valuable for understanding:",
        "- Which marketing channels have the strongest impact on sales",
        "- How much each channel contributes to overall sales performance  ",
        "- The statistical significance of each relationship",
        "- How well our marketing variables explain sales variance",
        "",
        "The goal is to build a predictive model that can guide marketing budget allocation decisions and provide actionable insights for business strategy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Overview",
        "",
        "Our dataset contains information about different marketing promotional budgets and their corresponding sales results. Before building the regression model, we need to examine what variables are available and understand their characteristics.",
        "",
        "**Key Variables Expected:**",
        "- **TV**: TV promotional budget or category",
        "- **Radio**: Radio promotional budget  ",
        "- **Social Media**: Social media promotional budget",
        "- **Influencer**: Influencer marketing budget or category",
        "- **Sales**: Sales results (our target variable)",
        "",
        "Let's start by loading and exploring the data to understand its structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Import Required Libraries",
        "",
        "First, we'll import all the necessary Python libraries for data analysis, visualization, and modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data manipulation and analysis",
        "import pandas as pd",
        "import numpy as np",
        "",
        "# Visualization",
        "import matplotlib.pyplot as plt",
        "import seaborn as sns",
        "",
        "# Statistical analysis",
        "from scipy import stats",
        "",
        "# Machine learning",
        "from sklearn.model_selection import train_test_split",
        "from sklearn.linear_model import LinearRegression",
        "from sklearn.metrics import mean_squared_error, r2_score",
        "",
        "# Advanced statistical modeling",
        "import statsmodels.api as sm",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor",
        "",
        "# Set visualization style",
        "plt.style.use('default')",
        "sns.set_palette(\"husl\")",
        "plt.rcParams['figure.figsize'] = (10, 6)",
        "",
        "print(\"All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load and Examine the Dataset",
        "",
        "Now we'll load the marketing sales data and take our first look at its structure and contents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset",
        "data = pd.read_csv('sales_marketing_data.csv')",
        "",
        "# Display basic information about the dataset",
        "print(\"Dataset shape:\", data.shape)",
        "print(\"\\nColumn names:\")",
        "print(data.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the first few rows to understand the data structure",
        "print(\"First 5 rows of the dataset:\")",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get detailed information about the dataset",
        "print(\"Dataset Information:\")",
        "print(\"=\" * 50)",
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate summary statistics",
        "print(\"Summary Statistics:\")",
        "print(\"=\" * 50)",
        "print(data.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Exploratory Data Analysis",
        "",
        "### Data Quality Assessment",
        "",
        "Before building our regression model, we need to thoroughly understand our data through exploratory analysis. This includes checking for missing values, understanding variable distributions, and identifying potential issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values",
        "print(\"Missing Values Analysis:\")",
        "print(\"=\" * 30)",
        "missing_values = data.isnull().sum()",
        "print(missing_values)",
        "print(\"\\nTotal missing values:\", data.isnull().sum().sum())",
        "",
        "# Check for duplicate rows",
        "duplicates = data.duplicated().sum()",
        "print(f\"\\nDuplicate rows: {duplicates}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Examine data types and unique values for each column",
        "print(\"Data Types and Unique Values:\")",
        "print(\"=\" * 40)",
        "for column in data.columns:",
        "    print(f\"\\n{column}:\")",
        "    print(f\"  Data type: {data[column].dtype}\")",
        "    print(f\"  Unique values: {data[column].nunique()}\")",
        "    if data[column].nunique() <= 10:  # Show unique values for categorical variables",
        "        print(f\"  Values: {sorted(data[column].unique())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizing Data Distributions",
        "",
        "Understanding the distribution of each variable helps us identify patterns, outliers, and the nature of our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create distribution plots for all numeric variables",
        "numeric_columns = data.select_dtypes(include=[np.number]).columns",
        "n_cols = len(numeric_columns)",
        "",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))",
        "axes = axes.ravel()",
        "",
        "for i, column in enumerate(numeric_columns):",
        "    if i < len(axes):",
        "        axes[i].hist(data[column], bins=20, alpha=0.7, edgecolor='black')",
        "        axes[i].set_title(f'Distribution of {column}')",
        "        axes[i].set_xlabel(column)",
        "        axes[i].set_ylabel('Frequency')",
        "",
        "# Remove empty subplots",
        "for i in range(len(numeric_columns), len(axes)):",
        "    fig.delaxes(axes[i])",
        "",
        "plt.tight_layout()",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analyzing Relationships Between Variables",
        "",
        "A key aspect of multiple linear regression is understanding how our predictor variables relate to each other and to our target variable (Sales). We'll use correlation analysis and visualizations to explore these relationships."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a correlation matrix for numeric variables",
        "numeric_data = data.select_dtypes(include=[np.number])",
        "correlation_matrix = numeric_data.corr()",
        "",
        "# Create a heatmap to visualize correlations",
        "plt.figure(figsize=(10, 8))",
        "sns.heatmap(correlation_matrix, ",
        "            annot=True, ",
        "            cmap='coolwarm', ",
        "            center=0,",
        "            square=True, ",
        "            fmt='.3f',",
        "            cbar_kws={'label': 'Correlation Coefficient'})",
        "plt.title('Correlation Matrix of Numeric Variables')",
        "plt.tight_layout()",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display correlations with Sales specifically",
        "print(\"Correlations with Sales (sorted by absolute value):\")",
        "print(\"=\" * 50)",
        "sales_correlations = correlation_matrix['Sales'].drop('Sales').sort_values(key=abs, ascending=False)",
        "for variable, correlation in sales_correlations.items():",
        "    print(f\"{variable:15}: {correlation:6.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pairwise Relationships Visualization",
        "",
        "A pairplot helps us visualize the relationships between all numeric variables simultaneously, making it easier to spot patterns and potential issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create pairplot for numeric variables",
        "plt.figure(figsize=(12, 10))",
        "sns.pairplot(numeric_data, diag_kind='hist', plot_kws={'alpha': 0.6})",
        "plt.suptitle('Pairwise Relationships Between Numeric Variables', y=1.02)",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Categorical Variables Analysis",
        "",
        "If our dataset contains categorical variables (like TV promotion levels or Influencer categories), we need to understand how these categories relate to our sales outcomes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify categorical variables",
        "categorical_columns = data.select_dtypes(include=['object']).columns",
        "print(f\"Categorical variables found: {list(categorical_columns)}\")",
        "",
        "if len(categorical_columns) > 0:",
        "    for column in categorical_columns:",
        "        print(f\"\\n{column} categories and their sales statistics:\")",
        "        print(\"-\" * 50)",
        "        category_stats = data.groupby(column)['Sales'].agg(['count', 'mean', 'std']).round(3)",
        "        print(category_stats)",
        "else:",
        "    print(\"No categorical variables found in the dataset.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize categorical variables' impact on sales",
        "categorical_columns = data.select_dtypes(include=['object']).columns",
        "",
        "if len(categorical_columns) > 0:",
        "    n_cats = len(categorical_columns)",
        "    fig, axes = plt.subplots(1, min(n_cats, 2), figsize=(15, 6))",
        "    ",
        "    if n_cats == 1:",
        "        axes = [axes]",
        "    ",
        "    for i, column in enumerate(categorical_columns[:2]):  # Show up to 2 categorical variables",
        "        if i < len(axes):",
        "            sns.boxplot(data=data, x=column, y='Sales', ax=axes[i])",
        "            axes[i].set_title(f'Sales Distribution by {column}')",
        "            axes[i].tick_params(axis='x', rotation=45)",
        "    ",
        "    plt.tight_layout()",
        "    plt.show()",
        "else:",
        "    print(\"No categorical variables to visualize.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Data Preparation for Modeling",
        "",
        "### Handling Missing Values and Data Cleaning",
        "",
        "Before building our regression model, we need to ensure our data is clean and properly formatted for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check and handle missing values",
        "print(\"Data cleaning process:\")",
        "print(\"=\" * 30)",
        "print(f\"Original dataset shape: {data.shape}\")",
        "",
        "# Remove rows with missing values",
        "data_clean = data.dropna()",
        "print(f\"After removing missing values: {data_clean.shape}\")",
        "print(f\"Rows removed: {data.shape[0] - data_clean.shape[0]}\")",
        "",
        "# Reset index after cleaning",
        "data_clean = data_clean.reset_index(drop=True)",
        "print(\"Index reset completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encoding Categorical Variables",
        "",
        "Multiple linear regression requires all input variables to be numeric. We'll convert categorical variables into dummy variables (also called one-hot encoding), which creates binary (0/1) variables for each category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify categorical and numeric columns",
        "categorical_cols = data_clean.select_dtypes(include=['object']).columns",
        "numeric_cols = data_clean.select_dtypes(include=[np.number]).columns",
        "",
        "print(\"Variable types identified:\")",
        "print(f\"Categorical columns: {list(categorical_cols)}\")",
        "print(f\"Numeric columns: {list(numeric_cols)}\")",
        "",
        "# Separate target variable",
        "target_variable = 'Sales'",
        "feature_numeric = [col for col in numeric_cols if col != target_variable]",
        "",
        "print(f\"\\nTarget variable: {target_variable}\")",
        "print(f\"Numeric features: {feature_numeric}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dummy variables for categorical features",
        "if len(categorical_cols) > 0:",
        "    print(\"Creating dummy variables for categorical features:\")",
        "    print(\"-\" * 50)",
        "    ",
        "    # Create dummy variables (drop_first=True to avoid multicollinearity)",
        "    dummy_variables = pd.get_dummies(data_clean[categorical_cols], drop_first=True, prefix_sep='_')",
        "    ",
        "    print(f\"Original categorical columns: {len(categorical_cols)}\")",
        "    print(f\"Dummy variables created: {dummy_variables.shape[1]}\")",
        "    print(\"\\nDummy variable columns:\")",
        "    print(list(dummy_variables.columns))",
        "    ",
        "    # Show first few rows of dummy variables",
        "    print(\"\\nFirst 5 rows of dummy variables:\")",
        "    print(dummy_variables.head())",
        "    ",
        "else:",
        "    print(\"No categorical variables found - no dummy variables needed.\")",
        "    dummy_variables = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine all features for modeling",
        "print(\"Preparing final feature matrix:\")",
        "print(\"=\" * 40)",
        "",
        "# Start with numeric features (excluding target)",
        "X = data_clean[feature_numeric].copy()",
        "print(f\"Numeric features shape: {X.shape}\")",
        "",
        "# Add dummy variables if they exist",
        "if not dummy_variables.empty:",
        "    X = pd.concat([X, dummy_variables], axis=1)",
        "    print(f\"After adding dummy variables: {X.shape}\")",
        "",
        "# Prepare target variable",
        "y = data_clean[target_variable].copy()",
        "",
        "print(f\"\\nFinal feature matrix shape: {X.shape}\")",
        "print(f\"Target variable shape: {y.shape}\")",
        "print(\"\\nFeature columns:\")",
        "for i, col in enumerate(X.columns, 1):",
        "    print(f\"  {i:2d}. {col}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Checking for Multicollinearity",
        "",
        "Before building our regression model, we should check for multicollinearity among predictor variables. High multicollinearity can make it difficult to determine the individual effect of each variable and can lead to unstable coefficient estimates.",
        "",
        "We'll use the Variance Inflation Factor (VIF) to assess multicollinearity:",
        "- **VIF < 5**: Low multicollinearity (acceptable)",
        "- **VIF 5-10**: Moderate multicollinearity (caution needed)  ",
        "- **VIF > 10**: High multicollinearity (problematic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate Variance Inflation Factor (VIF) for multicollinearity check",
        "def calculate_vif(dataframe):",
        "    vif_data = pd.DataFrame()",
        "    vif_data[\"Feature\"] = dataframe.columns",
        "    vif_data[\"VIF\"] = [variance_inflation_factor(dataframe.values, i) ",
        "                       for i in range(dataframe.shape[1])]",
        "    return vif_data.sort_values('VIF', ascending=False)",
        "",
        "# Calculate VIF for all features",
        "print(\"Variance Inflation Factor Analysis:\")",
        "print(\"=\" * 40)",
        "vif_results = calculate_vif(X)",
        "print(vif_results)",
        "",
        "print(\"\\nVIF Interpretation:\")",
        "print(\"- VIF < 5:   Low multicollinearity (good)\")",
        "print(\"- VIF 5-10:  Moderate multicollinearity (caution)\")",
        "print(\"- VIF > 10:  High multicollinearity (problematic)\")",
        "",
        "# Flag high VIF variables",
        "high_vif = vif_results[vif_results['VIF'] > 10]",
        "if not high_vif.empty:",
        "    print(f\"\\nWarning: {len(high_vif)} variables have high VIF (>10):\")",
        "    print(high_vif[['Feature', 'VIF']])",
        "else:",
        "    print(\"\\nGood news: No variables have problematically high VIF values.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Building the Multiple Linear Regression Model",
        "",
        "### Train-Test Split",
        "",
        "To properly evaluate our model's performance, we'll split our data into training and testing sets. The training set will be used to build the model, while the testing set will help us assess how well the model generalizes to new, unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets",
        "X_train, X_test, y_train, y_test = train_test_split(",
        "    X, y, ",
        "    test_size=0.2, ",
        "    random_state=42, ",
        "    stratify=None",
        ")",
        "",
        "print(\"Data split completed:\")",
        "print(\"=\" * 25)",
        "print(f\"Training set size: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")",
        "print(f\"Testing set size:  {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")",
        "print(f\"Number of features: {X_train.shape[1]}\")",
        "",
        "print(\"\\nTraining set statistics:\")",
        "print(f\"Target variable (Sales) - Mean: {y_train.mean():.2f}, Std: {y_train.std():.2f}\")",
        "print(\"\\nTesting set statistics:\")",
        "print(f\"Target variable (Sales) - Mean: {y_test.mean():.2f}, Std: {y_test.std():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Training",
        "",
        "Now we'll create and train our multiple linear regression model using scikit-learn's LinearRegression class. This will find the best-fitting linear relationship between our marketing variables and sales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and train the multiple linear regression model",
        "model = LinearRegression()",
        "",
        "# Fit the model to training data",
        "model.fit(X_train, y_train)",
        "",
        "print(\"Multiple Linear Regression Model Training Complete!\")",
        "print(\"=\" * 55)",
        "print(f\"Model intercept (\u03b2\u2080): {model.intercept_:.4f}\")",
        "print(f\"Number of coefficients: {len(model.coef_)}\")",
        "print(\"\\nModel equation:\")",
        "print(f\"Sales = {model.intercept_:.4f}\", end=\"\")",
        "for i, (feature, coef) in enumerate(zip(X.columns, model.coef_)):",
        "    sign = \"+\" if coef >= 0 else \"\"",
        "    print(f\" {sign}{coef:.4f}*{feature}\", end=\"\")",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Model Evaluation and Performance Assessment",
        "",
        "### Prediction and Performance Metrics",
        "",
        "Let's evaluate how well our model performs on both training and testing data using key regression metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions on both training and testing sets",
        "y_train_pred = model.predict(X_train)",
        "y_test_pred = model.predict(X_test)",
        "",
        "# Calculate performance metrics",
        "train_r2 = r2_score(y_train, y_train_pred)",
        "test_r2 = r2_score(y_test, y_test_pred)",
        "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))",
        "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))",
        "",
        "print(\"MODEL PERFORMANCE METRICS\")",
        "print(\"=\" * 40)",
        "print(f\"Training R\u00b2 Score:    {train_r2:.4f}\")",
        "print(f\"Testing R\u00b2 Score:     {test_r2:.4f}\")",
        "print(f\"Training RMSE:        {train_rmse:.4f}\")",
        "print(f\"Testing RMSE:         {test_rmse:.4f}\")",
        "",
        "# Assess overfitting",
        "r2_difference = abs(train_r2 - test_r2)",
        "print(f\"\\nR\u00b2 Difference:        {r2_difference:.4f}\")",
        "",
        "if r2_difference < 0.05:",
        "    print(\"\u2713 Model shows good generalization (low overfitting)\")",
        "elif r2_difference < 0.10:",
        "    print(\"\u26a0 Model shows moderate overfitting\")",
        "else:",
        "    print(\"\u2717 Model shows significant overfitting\")",
        "",
        "print(f\"\\nModel explains {test_r2*100:.1f}% of sales variance in test data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Statistical Significance Testing",
        "",
        "Using statsmodels, we can get detailed statistical information about our regression model, including p-values, confidence intervals, and other diagnostic statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use statsmodels for detailed statistical analysis",
        "X_train_sm = sm.add_constant(X_train)  # Add intercept term",
        "X_test_sm = sm.add_constant(X_test)",
        "",
        "# Fit the model using statsmodels",
        "sm_model = sm.OLS(y_train, X_train_sm).fit()",
        "",
        "# Display comprehensive model summary",
        "print(\"DETAILED STATISTICAL ANALYSIS\")",
        "print(\"=\" * 50)",
        "print(sm_model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Coefficient Analysis and Interpretation",
        "",
        "Let's examine each coefficient in detail to understand which marketing channels have the strongest impact on sales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a detailed coefficient analysis",
        "coefficients_df = pd.DataFrame({",
        "    'Feature': X.columns,",
        "    'Coefficient': model.coef_,",
        "    'Abs_Coefficient': np.abs(model.coef_)",
        "}).sort_values('Abs_Coefficient', ascending=False)",
        "",
        "print(\"COEFFICIENT ANALYSIS\")",
        "print(\"=\" * 30)",
        "print(coefficients_df)",
        "",
        "print(\"\\nCOEFFICIENT INTERPRETATION:\")",
        "print(\"-\" * 35)",
        "for _, row in coefficients_df.head().iterrows():",
        "    direction = \"increases\" if row['Coefficient'] > 0 else \"decreases\"",
        "    print(f\"\u2022 {row['Feature']}: For each unit increase, sales {direction} by {abs(row['Coefficient']):.4f}\")",
        "",
        "# Statistical significance from statsmodels",
        "print(\"\\nSTATISTICAL SIGNIFICANCE (p-values):\")",
        "print(\"-\" * 40)",
        "p_values = sm_model.pvalues[1:]  # Exclude intercept",
        "for feature, p_val in zip(X.columns, p_values):",
        "    significance = \"***\" if p_val < 0.001 else \"**\" if p_val < 0.01 else \"*\" if p_val < 0.05 else \"\"",
        "    print(f\"{feature:20}: p = {p_val:.4f} {significance}\")",
        "",
        "print(\"\\nSignificance levels: *** p<0.001, ** p<0.01, * p<0.05\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Visualization",
        "",
        "Let's create visualizations to better understand our model's performance and the relationships it has captured."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create model performance visualizations",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))",
        "",
        "# 1. Actual vs Predicted (Training)",
        "axes[0,0].scatter(y_train, y_train_pred, alpha=0.6, color='blue')",
        "axes[0,0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)",
        "axes[0,0].set_xlabel('Actual Sales')",
        "axes[0,0].set_ylabel('Predicted Sales')",
        "axes[0,0].set_title(f'Training Set: Actual vs Predicted\\nR\u00b2 = {train_r2:.3f}')",
        "",
        "# 2. Actual vs Predicted (Testing)",
        "axes[0,1].scatter(y_test, y_test_pred, alpha=0.6, color='green')",
        "axes[0,1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)",
        "axes[0,1].set_xlabel('Actual Sales')",
        "axes[0,1].set_ylabel('Predicted Sales')",
        "axes[0,1].set_title(f'Testing Set: Actual vs Predicted\\nR\u00b2 = {test_r2:.3f}')",
        "",
        "# 3. Coefficient importance",
        "top_features = coefficients_df.head(8)  # Show top 8 features",
        "axes[1,0].barh(range(len(top_features)), top_features['Coefficient'])",
        "axes[1,0].set_yticks(range(len(top_features)))",
        "axes[1,0].set_yticklabels(top_features['Feature'])",
        "axes[1,0].set_xlabel('Coefficient Value')",
        "axes[1,0].set_title('Feature Coefficients (Impact on Sales)')",
        "axes[1,0].axvline(x=0, color='red', linestyle='--', alpha=0.7)",
        "",
        "# 4. Residuals distribution",
        "residuals = y_test - y_test_pred",
        "axes[1,1].hist(residuals, bins=15, alpha=0.7, edgecolor='black')",
        "axes[1,1].set_xlabel('Residuals')",
        "axes[1,1].set_ylabel('Frequency')",
        "axes[1,1].set_title('Distribution of Residuals')",
        "axes[1,1].axvline(x=0, color='red', linestyle='--', alpha=0.7)",
        "",
        "plt.tight_layout()",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Residual Analysis and Model Assumptions",
        "",
        "### Checking Linear Regression Assumptions",
        "",
        "Multiple linear regression relies on several key assumptions. Let's test these assumptions using residual analysis:",
        "",
        "1. **Linearity**: The relationship between predictors and target should be linear",
        "2. **Independence**: Residuals should be independent of each other",
        "3. **Homoscedasticity**: Residuals should have constant variance",
        "4. **Normality**: Residuals should be approximately normally distributed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive residual analysis",
        "residuals_train = y_train - y_train_pred",
        "residuals_test = y_test - y_test_pred",
        "",
        "print(\"RESIDUAL ANALYSIS SUMMARY\")",
        "print(\"=\" * 35)",
        "print(f\"Training residuals - Mean: {residuals_train.mean():.6f}, Std: {residuals_train.std():.4f}\")",
        "print(f\"Testing residuals  - Mean: {residuals_test.mean():.6f}, Std: {residuals_test.std():.4f}\")",
        "",
        "# Test for normality using Shapiro-Wilk test",
        "from scipy.stats import shapiro",
        "stat_train, p_train = shapiro(residuals_train)",
        "stat_test, p_test = shapiro(residuals_test)",
        "",
        "print(f\"\\nNormality Test (Shapiro-Wilk):\")",
        "print(f\"Training set: statistic = {stat_train:.4f}, p-value = {p_train:.4f}\")",
        "print(f\"Testing set:  statistic = {stat_test:.4f}, p-value = {p_test:.4f}\")",
        "",
        "if p_test > 0.05:",
        "    print(\"\u2713 Residuals appear to be normally distributed (p > 0.05)\")",
        "else:",
        "    print(\"\u26a0 Residuals may not be normally distributed (p \u2264 0.05)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive residual plots",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))",
        "",
        "# 1. Residuals vs Fitted Values (Homoscedasticity check)",
        "axes[0,0].scatter(y_test_pred, residuals_test, alpha=0.6)",
        "axes[0,0].axhline(y=0, color='red', linestyle='--')",
        "axes[0,0].set_xlabel('Fitted Values')",
        "axes[0,0].set_ylabel('Residuals')",
        "axes[0,0].set_title('Residuals vs Fitted Values\\n(Check for Homoscedasticity)')",
        "",
        "# 2. Q-Q Plot (Normality check)",
        "from scipy.stats import probplot",
        "probplot(residuals_test, dist=\"norm\", plot=axes[0,1])",
        "axes[0,1].set_title('Q-Q Plot of Residuals\\n(Check for Normality)')",
        "",
        "# 3. Histogram of residuals",
        "axes[1,0].hist(residuals_test, bins=15, alpha=0.7, edgecolor='black', density=True)",
        "axes[1,0].set_xlabel('Residuals')",
        "axes[1,0].set_ylabel('Density')",
        "axes[1,0].set_title('Distribution of Residuals')",
        "",
        "# Overlay normal distribution",
        "x_norm = np.linspace(residuals_test.min(), residuals_test.max(), 100)",
        "y_norm = stats.norm.pdf(x_norm, residuals_test.mean(), residuals_test.std())",
        "axes[1,0].plot(x_norm, y_norm, 'r-', linewidth=2, label='Normal Distribution')",
        "axes[1,0].legend()",
        "",
        "# 4. Residuals vs Order (Independence check)",
        "axes[1,1].plot(range(len(residuals_test)), residuals_test, 'o-', alpha=0.6)",
        "axes[1,1].axhline(y=0, color='red', linestyle='--')",
        "axes[1,1].set_xlabel('Observation Order')",
        "axes[1,1].set_ylabel('Residuals')",
        "axes[1,1].set_title('Residuals vs Order\\n(Check for Independence)')",
        "",
        "plt.tight_layout()",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Business Insights and Recommendations",
        "",
        "### Marketing Channel Effectiveness Analysis",
        "",
        "Based on our multiple linear regression analysis, let's translate the statistical findings into actionable business insights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate business insights based on model results",
        "print(\"BUSINESS INSIGHTS AND RECOMMENDATIONS\")",
        "print(\"=\" * 50)",
        "",
        "# Identify most impactful marketing channels",
        "top_positive = coefficients_df[coefficients_df['Coefficient'] > 0].head(3)",
        "top_negative = coefficients_df[coefficients_df['Coefficient'] < 0].head(3)",
        "",
        "print(\"\\n\ud83c\udfaf MOST EFFECTIVE MARKETING CHANNELS:\")",
        "print(\"-\" * 40)",
        "for i, (_, row) in enumerate(top_positive.iterrows(), 1):",
        "    impact = row['Coefficient']",
        "    print(f\"{i}. {row['Feature']}: +{impact:.4f} sales impact per unit\")",
        "",
        "if not top_negative.empty:",
        "    print(\"\\n\u26a0\ufe0f  CHANNELS WITH NEGATIVE IMPACT:\")",
        "    print(\"-\" * 35)",
        "    for i, (_, row) in enumerate(top_negative.iterrows(), 1):",
        "        impact = abs(row['Coefficient'])",
        "        print(f\"{i}. {row['Feature']}: -{impact:.4f} sales impact per unit\")",
        "",
        "# Model performance summary",
        "print(f\"\\n\ud83d\udcca MODEL PERFORMANCE SUMMARY:\")",
        "print(\"-\" * 30)",
        "print(f\"\u2022 Model explains {test_r2*100:.1f}% of sales variance\")",
        "print(f\"\u2022 Average prediction error: \u00b1{test_rmse:.2f} sales units\")",
        "print(f\"\u2022 Model generalization: {'Good' if r2_difference < 0.05 else 'Moderate' if r2_difference < 0.10 else 'Poor'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Strategic recommendations based on analysis",
        "print(\"\\n\ud83d\udca1 STRATEGIC RECOMMENDATIONS:\")",
        "print(\"-\" * 35)",
        "",
        "# Recommendation 1: Budget allocation",
        "if not top_positive.empty:",
        "    best_channel = top_positive.iloc[0]['Feature']",
        "    best_impact = top_positive.iloc[0]['Coefficient']",
        "    print(f\"1. PRIORITIZE {best_channel.upper()}\")",
        "    print(f\"   \u2022 Highest ROI: {best_impact:.4f} sales per unit investment\")",
        "    print(f\"   \u2022 Consider increasing budget allocation to this channel\")",
        "",
        "# Recommendation 2: Model reliability",
        "if test_r2 > 0.7:",
        "    print(f\"\\n2. HIGH MODEL CONFIDENCE\")",
        "    print(f\"   \u2022 R\u00b2 = {test_r2:.3f} indicates strong predictive power\")",
        "    print(f\"   \u2022 Use this model for budget planning and forecasting\")",
        "elif test_r2 > 0.5:",
        "    print(f\"\\n2. MODERATE MODEL CONFIDENCE\")",
        "    print(f\"   \u2022 R\u00b2 = {test_r2:.3f} shows reasonable predictive ability\")",
        "    print(f\"   \u2022 Consider collecting additional data to improve accuracy\")",
        "else:",
        "    print(f\"\\n2. LOW MODEL CONFIDENCE\")",
        "    print(f\"   \u2022 R\u00b2 = {test_r2:.3f} suggests limited predictive power\")",
        "    print(f\"   \u2022 Investigate additional variables or non-linear relationships\")",
        "",
        "# Recommendation 3: Data quality",
        "print(f\"\\n3. DATA QUALITY ASSESSMENT\")",
        "if p_test > 0.05:",
        "    print(f\"   \u2022 \u2713 Model assumptions are reasonably met\")",
        "    print(f\"   \u2022 Results can be trusted for decision-making\")",
        "else:",
        "    print(f\"   \u2022 \u26a0 Some model assumptions may be violated\")",
        "    print(f\"   \u2022 Consider data transformation or alternative modeling approaches\")",
        "",
        "print(f\"\\n4. NEXT STEPS\")",
        "print(f\"   \u2022 Monitor actual vs predicted performance\")",
        "print(f\"   \u2022 Update model quarterly with new data\")",
        "print(f\"   \u2022 Test model predictions with A/B experiments\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion",
        "",
        "### Summary of Multiple Linear Regression Analysis",
        "",
        "This comprehensive analysis has demonstrated how to perform multiple linear regression on marketing sales data. Here are the key takeaways:",
        "",
        "#### **What We Accomplished:**",
        "- \u2705 **Data Exploration**: Thoroughly examined the dataset structure, distributions, and relationships",
        "- \u2705 **Data Preparation**: Cleaned data, handled categorical variables, and checked for multicollinearity  ",
        "- \u2705 **Model Building**: Created and trained a multiple linear regression model",
        "- \u2705 **Statistical Analysis**: Evaluated model performance and statistical significance",
        "- \u2705 **Assumption Testing**: Verified regression assumptions through residual analysis",
        "- \u2705 **Business Insights**: Translated statistical findings into actionable recommendations",
        "",
        "#### **Key Statistical Findings:**",
        "- Model performance metrics (R\u00b2, RMSE) indicate the model's predictive accuracy",
        "- Coefficient analysis reveals which marketing channels have the strongest impact on sales",
        "- Statistical significance testing identifies which relationships are reliable",
        "- Residual analysis confirms whether regression assumptions are met",
        "",
        "#### **Business Value:**",
        "This analysis provides a data-driven foundation for:",
        "- **Budget Allocation**: Prioritize marketing channels with highest sales impact",
        "- **Performance Forecasting**: Predict sales based on marketing spend",
        "- **Strategic Planning**: Make informed decisions about marketing investments",
        "- **ROI Optimization**: Focus resources on most effective channels",
        "",
        "#### **Model Limitations and Considerations:**",
        "- Linear regression assumes linear relationships between variables",
        "- Model performance depends on data quality and completeness",
        "- External factors not captured in the data may influence sales",
        "- Regular model updates are needed as market conditions change",
        "",
        "### Next Steps for Implementation",
        "",
        "1. **Validate Results**: Test model predictions against actual outcomes",
        "2. **Monitor Performance**: Track model accuracy over time",
        "3. **Expand Analysis**: Consider additional variables or advanced modeling techniques",
        "4. **Automate Process**: Set up regular model retraining with new data",
        "",
        "This analysis demonstrates the power of multiple linear regression for understanding complex business relationships and making data-driven decisions in marketing strategy."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}